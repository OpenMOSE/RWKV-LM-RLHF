pytorch-lightning==2.1.3
deepspeed==0.14.5
bitsandbytes
wandb
prompt_toolkit
pandas
rwkv
triton
transformers
einops
h5py
torchao
safetensors
git+https://github.com/fla-org/flash-linear-attention